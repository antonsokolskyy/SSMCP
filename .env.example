# Docker Compose Environment Selection
# To run in Development mode, uncomment the following line:
# COMPOSE_FILE=docker-compose.yml:docker-compose.dev.yml
#
# To run in Production mode (default), leave it commented or set to:
# COMPOSE_FILE=docker-compose.yml

# Debug mode (true | false, default: false)
# When true, debug logs will be shown.
SSMCP_DEBUG=false

# UV Configuration
# Set the link mode for uv package manager (copy vs hardlink). 'copy' is recommended for Docker.
UV_LINK_MODE=copy

# URL endpoint for SearXNG search API (required)
SEARXNG_SEARCH_URL=http://searxng:8080/search
# Maximum number of search results to return (up to 20, default: 5)
SEARXNG_MAX_RESULTS=5
# Request timeout for SearXNG service (default: 5.0)
SEARXNG_TIMEOUT=5.0

# Language for YouTube subtitles/captions (e.g., 'en', 'es', 'fr', default: 'en')
YOUTUBE_SUBTITLE_LANGUAGE=en
# Path to YouTube cookies file for age-restricted content (default: /app/deploy/docker/ssmcp/cookies.txt)
YOUTUBE_COOKIES_PATH=/app/deploy/docker/ssmcp/cookies.txt

# Host address to bind the MCP server (all interfaces, default: 0.0.0.0)
HOST=0.0.0.0
# Port number for the MCP server (default: 8000)
PORT=8000

# TOOL DESCRIPTIONS
# Description for the web_search tool
TOOL_WEB_SEARCH_DESC="Perform a web search and return relevant results.\n\nEach search result contains URL and page content in MD format"
# Description for the web_fetch tool
TOOL_WEB_FETCH_DESC="Fetch content from a specified URL.\n\nReturns the page content in Markdown format."
# Description for the youtube_get_subtitles tool
TOOL_YOUTUBE_GET_SUBTITLES_DESC="Get subtitles/captions from a YouTube video"

# TOOL ARGUMENT DESCRIPTIONS
# Description for the query argument of web_search tool
ARG_WEB_SEARCH_QUERY_DESC="Search query or keywords to find relevant web content."
# Description for the url argument of web_fetch tool
ARG_WEB_FETCH_URL_DESC="The URL to fetch content from"
# Description for the url argument of youtube_get_subtitles tool
ARG_YOUTUBE_GET_SUBTITLES_URL_DESC="YouTube video URL to get subtitles from"


# CRAWL4AI CONFIGURATION
# https://docs.crawl4ai.com/api/parameters/

# Initial page width (in px). Useful for testing responsive layouts. (default: 1280)
CRAWL4AI_VIEWPORT_WIDTH=1280
# Initial page height (in px). (default: 900)
CRAWL4AI_VIEWPORT_HEIGHT=900
# Number of browser instances to keep in the pool (default: 5)
CRAWL4AI_BROWSER_POOL_SIZE=5
# Condition for navigation to "complete". "networkidle" or "domcontentloaded". (default "domcontentloaded")
CRAWL4AI_WAIT_UNTIL=domcontentloaded
# Timeout for page navigation or JS steps. Increase for slow sites. (default: 10000)
CRAWL4AI_PAGE_TIMEOUT=10000
# Maximum number of scroll steps during full page scan. If None, scrolls until entire page is loaded. (default: 0)
CRAWL4AI_MAX_SCROLL_STEPS=0
# Delay between scroll steps if scan_full_page=True.(default: 0.5)
CRAWL4AI_SCROLL_DELAY=0.5
# Additional pause (seconds) before final HTML is captured. Good for last-second updates. (default: 0.5)
CRAWL4AI_DELAY_BEFORE_RETURN_HTML=0.5
# Cache mode: "enabled" (read/write), "disabled" (no cache), or "bypass" (write only). (default "enabled")
CRAWL4AI_CACHE_MODE=enabled
# Skips text blocks below X words. Helps ignore trivial sections. (default: 1)
CRAWL4AI_WORD_COUNT_THRESHOLD=1
# If a block has fewer words than this, it's pruned. (default: 1)
CRAWL4AI_MIN_WORD_THRESHOLD=1
# Content pruning threshold - higher = more aggressive filtering (range: 0.0-1.0, default: 0.30)
# Controls sensitivity of the PruningContentFilter
CRAWL4AI_PRUNING_THRESHOLD=0.3
# Affects pruning behavior (default: dynamic)
# "fixed" each node must exceed threshold (0â€“1).
# "dynamic" node scoring adjusts according to tag type, text/link density, etc.
CRAWL4AI_THRESHOLD_TYPE=dynamic
# Comma-separated list of HTML tags to exclude from content (default: nav,footer,header,aside,script,style,noscript,form,button,iframe,svg,meta)
# Removes entire tags and their content
CRAWL4AI_EXCLUDED_TAGS=nav,footer,header,aside,script,style,noscript,form,button,iframe,svg,meta
# Minimum score threshold for processing a table. Lower values include more tables. (default: 1)
CRAWL4AI_TABLE_SCORE_THRESHOLD=1
# Removes all links pointing outside the current domain. (CrawlerRunConfig) (default: true)
CRAWL4AI_EXCLUDE_EXTERNAL_LINKS=true
# Whether to remove all hyperlinks in the final markdown. (MarkdownGenerator config) (default: true)
CRAWL4AI_IGNORE_LINKS=true
# If True, omit #localAnchors or internal links referencing the same page. (default: true)
CRAWL4AI_SKIP_INTERNAL_LINKS=true
# Whether to remove all images from markdown output (default: true)
CRAWL4AI_IGNORE_IMAGES=true
# Whether to escape HTML entities in markdown (default: true)
CRAWL4AI_ESCAPE_HTML=true
# Body width for markdown formatting, 0 = no wrapping (default: 0)
CRAWL4AI_BODY_WIDTH=0
# Include superscript and subscript tags in markdown (default: true)
CRAWL4AI_INCLUDE_SUP_SUB=true

# CSS SELECTOR FILTER CONFIGURATION
# Comma-separated list of CSS selectors to try in priority order for content extraction (default: article, main, [role="main"], .article, .article-content, .page-content, .markdown, #article, #content, #main, #page, .content)
CSS_SELECTOR_PRIORITY_LIST=article, main, [role="main"], .article, .article-content, .page-content, .markdown, #article, #content, #main, #page, .content
# Minimum word count required for selected content block (default: 50)
CSS_SELECTOR_MIN_WORDS=50

# HTML TYPE SELECTION
# Choose which HTML type to use for markdown conversion: "cleaned_html" or "fit_html" (default: "fit_html")
# - cleaned_html: Pre-filtered HTML (removes nav, footer, ads, etc.)
# - fit_html: More aggressively filtered/pruned HTML for better content focus
EXTRACTION_HTML_TYPE=fit_html

# REDIS CONFIGURATION (Optional)
# Redis URL for storing requests/responses (default: redis://redis:6379)
# REDIS_URL=redis://redis:6379
# Prefix for Redis keys (default: ssmcp)
REDIS_KEY_PREFIX=ssmcp
# Expiration time for Redis keys in seconds (default: 3600)
REDIS_EXPIRATION_SECONDS=3600
